{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "115bada4-ee4d-45be-9379-8e7d41434f86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from konlpy.tag import Okt\n",
    "\n",
    "okt = Okt()\n",
    "def tw_tokenizer(text):\n",
    "    # 입력 인자로 들어온 text 를 형태소 단어로 토큰화 하여 list 객체 반환\n",
    "    tokens_ko = okt.morphs(text)\n",
    "    return tokens_ko\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "59971c79-02ef-431b-84f1-798e3822fafc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['밥', '을', '많이', '먹어서', '화장실', '을', '가야', '할꺼', '같은데요']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token = okt.morphs('머리가 어지럽네요')\n",
    "token\n",
    "token = okt.morphs('밥을 많이 먹어서 화장실을 가야 할꺼 같은데요')\n",
    "token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c02b4fa6-6617-4b54-85d0-11a00d3fa4b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 0)\t1\n",
      "  (0, 3)\t1\n",
      "  (0, 4)\t1\n",
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n",
      "  (0, 9)\t1\n",
      "  (0, 10)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 12)\t1\n",
      "  (1, 0)\t1\n",
      "  (1, 3)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  (1, 8)\t1\n",
      "  (1, 11)\t1\n",
      "  (1, 12)\t1\n",
      "  (1, 13)\t1\n",
      "  (1, 14)\t1\n",
      "  (2, 0)\t1\n",
      "  (2, 1)\t1\n",
      "  (2, 2)\t1\n",
      "  (2, 3)\t1\n",
      "  (2, 4)\t1\n",
      "  (2, 5)\t1\n",
      "  (2, 6)\t1\n",
      "  (2, 11)\t1\n",
      "  (2, 12)\t1\n",
      "[[1 0 0 1 1 1 0 1 0 1 1 1 1 0 0]\n",
      " [1 0 0 1 1 1 0 0 1 0 0 1 1 1 1]\n",
      " [1 1 1 1 1 1 1 0 0 0 0 1 1 0 0]]\n",
      "{'나': 3, '는': 5, '선생님': 9, '입니다': 11, '.': 0, '나 는': 4, '는 선생님': 7, '선생님 입니다': 10, '입니다 .': 12, '학생': 13, '는 학생': 8, '학생 입니다': 14, '개발자': 1, '는 개발자': 6, '개발자 입니다': 2}\n"
     ]
    }
   ],
   "source": [
    "train_data = ['나는 선생님입니다.','나는 학생입니다.','나는 개발자 입니다.']\n",
    "\n",
    "cvt = CountVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2))\n",
    "cvt.fit(train_data)\n",
    "train_vec = cvt.transform(train_data)\n",
    "\n",
    "print(train_vec)\n",
    "print(train_vec.todense())\n",
    "print(cvt.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7cf2fbef-c38f-488e-90db-0f4ebd603035",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 12)\t0.2617097912180386\n",
      "  (0, 11)\t0.2617097912180386\n",
      "  (0, 10)\t0.44311319512575403\n",
      "  (0, 9)\t0.44311319512575403\n",
      "  (0, 7)\t0.44311319512575403\n",
      "  (0, 5)\t0.2617097912180386\n",
      "  (0, 4)\t0.2617097912180386\n",
      "  (0, 3)\t0.2617097912180386\n",
      "  (0, 0)\t0.2617097912180386\n",
      "  (1, 14)\t0.44311319512575403\n",
      "  (1, 13)\t0.44311319512575403\n",
      "  (1, 12)\t0.2617097912180386\n",
      "  (1, 11)\t0.2617097912180386\n",
      "  (1, 8)\t0.44311319512575403\n",
      "  (1, 5)\t0.2617097912180386\n",
      "  (1, 4)\t0.2617097912180386\n",
      "  (1, 3)\t0.2617097912180386\n",
      "  (1, 0)\t0.2617097912180386\n",
      "  (2, 12)\t0.2617097912180386\n",
      "  (2, 11)\t0.2617097912180386\n",
      "  (2, 6)\t0.44311319512575403\n",
      "  (2, 5)\t0.2617097912180386\n",
      "  (2, 4)\t0.2617097912180386\n",
      "  (2, 3)\t0.2617097912180386\n",
      "  (2, 2)\t0.44311319512575403\n",
      "  (2, 1)\t0.44311319512575403\n",
      "  (2, 0)\t0.2617097912180386\n",
      "[[0.26170979 0.         0.         0.26170979 0.26170979 0.26170979\n",
      "  0.         0.4431132  0.         0.4431132  0.4431132  0.26170979\n",
      "  0.26170979 0.         0.        ]\n",
      " [0.26170979 0.         0.         0.26170979 0.26170979 0.26170979\n",
      "  0.         0.         0.4431132  0.         0.         0.26170979\n",
      "  0.26170979 0.4431132  0.4431132 ]\n",
      " [0.26170979 0.4431132  0.4431132  0.26170979 0.26170979 0.26170979\n",
      "  0.4431132  0.         0.         0.         0.         0.26170979\n",
      "  0.26170979 0.         0.        ]]\n",
      "{'나': 3, '는': 5, '선생님': 9, '입니다': 11, '.': 0, '나 는': 4, '는 선생님': 7, '선생님 입니다': 10, '입니다 .': 12, '학생': 13, '는 학생': 8, '학생 입니다': 14, '개발자': 1, '는 개발자': 6, '개발자 입니다': 2}\n"
     ]
    }
   ],
   "source": [
    "train_data = ['나는 선생님입니다.','나는 학생입니다.','나는 개발자 입니다.']\n",
    "\n",
    "tfidf = TfidfVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2))\n",
    "tfidf.fit(train_data)\n",
    "train_vec = tfidf.transform(train_data)\n",
    "\n",
    "print(train_vec)\n",
    "print(train_vec.todense())\n",
    "print(tfidf.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "ef34f05b-791b-405c-adc9-5705826c1ad3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 3)\t1\n",
      "  (0, 8)\t1\n",
      "  (0, 11)\t1\n",
      "  (0, 26)\t1\n",
      "  (0, 27)\t1\n",
      "  (0, 28)\t1\n",
      "  (0, 29)\t1\n",
      "  (0, 30)\t1\n",
      "  (0, 31)\t1\n",
      "  (0, 50)\t1\n",
      "  (0, 51)\t1\n",
      "  (0, 67)\t1\n",
      "  (0, 68)\t1\n",
      "  (0, 74)\t1\n",
      "  (0, 75)\t1\n",
      "  (0, 76)\t1\n",
      "  (0, 77)\t1\n",
      "  (0, 80)\t1\n",
      "  (0, 81)\t1\n",
      "  (0, 82)\t1\n",
      "  (0, 83)\t1\n",
      "  (0, 96)\t1\n",
      "  (0, 97)\t1\n",
      "  (1, 4)\t1\n",
      "  (1, 5)\t1\n",
      "  :\t:\n",
      "  (6, 3)\t1\n",
      "  (6, 4)\t1\n",
      "  (6, 5)\t1\n",
      "  (6, 6)\t1\n",
      "  (6, 7)\t1\n",
      "  (6, 8)\t2\n",
      "  (6, 9)\t1\n",
      "  (6, 10)\t1\n",
      "  (6, 18)\t1\n",
      "  (6, 19)\t1\n",
      "  (6, 34)\t1\n",
      "  (6, 36)\t1\n",
      "  (6, 44)\t1\n",
      "  (6, 45)\t1\n",
      "  (6, 48)\t1\n",
      "  (6, 49)\t1\n",
      "  (6, 61)\t1\n",
      "  (6, 62)\t1\n",
      "  (6, 63)\t1\n",
      "  (6, 64)\t1\n",
      "  (6, 88)\t1\n",
      "  (6, 89)\t1\n",
      "  (7, 12)\t1\n",
      "  (7, 13)\t1\n",
      "  (7, 16)\t1\n",
      "[[0 0 0 1 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0\n",
      "  0 0 1 1 1 1 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0]\n",
      " [0 0 0 0 1 1 1 1 2 0 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [1 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 1 1\n",
      "  1 1 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 1 1 0\n",
      "  0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 1 1 0 0 1 0 1 0 0\n",
      "  0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 1 0 0 0 0]\n",
      " [0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 1 1\n",
      "  0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1]\n",
      " [0 0 0 1 1 1 1 1 2 1 1 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  1 0 0 0 0 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "{'뽀로로': 50, '자수': 82, '네': 28, '임택': 80, '어린이집': 67, '가방': 8, '이름': 74, '표': 96, '낮잠': 26, '이불': 76, '네임택': 30, '.': 3, '뽀로로 자수': 51, '자수 네': 83, '네 임택': 29, '임택 어린이집': 81, '어린이집 가방': 68, '가방 이름': 11, '이름 표': 75, '표 낮잠': 97, '낮잠 이불': 27, '이불 네임택': 77, '네임택 .': 31, '[': 4, '캐릭터': 88, ']': 6, '구름': 18, '속': 61, '시나': 63, '모': 44, '롤': 34, '보조': 48, '[ 캐릭터': 5, '캐릭터 가방': 89, '가방 ]': 10, '] 구름': 7, '구름 속': 19, '속 시나': 62, '시나 모': 64, '모 롤': 45, '롤 보조': 36, '보조 가방': 49, '패티': 92, '귀여운': 20, '석': 59, '고': 14, '방향': 46, '제': 84, '차량': 86, '용': 72, '(': 0, '리필': 38, '오일': 70, '포함': 94, ')': 2, '뽀로로 패티': 52, '패티 귀여운': 93, '귀여운 석': 21, '석 고': 60, '고 방향': 15, '방향 제': 47, '제 차량': 85, '차량 용': 87, '용 (': 73, '( 리필': 1, '리필 오일': 39, '오일 포함': 71, '포함 )': 95, '아기': 65, '고무신': 16, '아기 고무신': 66, '고무신 .': 17, '산리오': 55, '사탕': 53, '튤립': 90, '인형': 78, '꽃다발': 24, '답례': 32, '품': 98, '산리오 시나': 56, '롤 사탕': 37, '사탕 튤립': 54, '튤립 인형': 91, '인형 꽃다발': 79, '꽃다발 어린이집': 25, '어린이집 답례': 69, '답례 품': 33, '마이': 40, '멜로디': 42, '생일': 57, '기념일': 22, '헬륨': 101, '풍선': 99, '롤 마이': 35, '마이 멜로디': 41, '멜로디 생일': 43, '생일 기념일': 58, '기념일 헬륨': 23, '헬륨 풍선': 102, '풍선 .': 100, '가방 .': 9, '검정': 12, '검정 고무신': 13}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\KB\\PycharmProjects\\pythonProject\\venv\\Lib\\site-packages\\sklearn\\feature_extraction\\text.py:528: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "x_data = ['뽀로로 자수네임택 어린이집 가방 이름표 낮잠이불 네임택.',\n",
    "          '[캐릭터가방]구름속 시나모롤 보조가방',\n",
    "          '뽀로로 패티 귀여운 석고방향제 차량용(리필오일포함)',\n",
    "          '아기 고무신.',\n",
    "          '산리오 시나모롤 사탕 튤립 인형 꽃다발 어린이집 답례품',\n",
    "          '산리오 시나모롤 마이멜로디 생일 기념일 헬륨 풍선.',\n",
    "          '[캐릭터가방]구름속 시나모롤 보조가방.',\n",
    "          '검정 고무신']\n",
    "# 0 뽀로로 1 산리오 2 기타\n",
    "y_data = [0,1,0,2,1,1,1,2]\n",
    "\n",
    "cvt = CountVectorizer(tokenizer=tw_tokenizer, ngram_range=(1,2))\n",
    "cvt.fit(x_data)\n",
    "x_train = cvt.transform(x_data)\n",
    "\n",
    "print(x_train)\n",
    "print(x_train.todense())\n",
    "print(cvt.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "8e616a4b-6108-4e34-b42e-e43f5e7a9fe9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train.shape\n",
    "len(y_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "665d6246-e840-4ced-9607-0e949a1b33d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 1 0 2 1 1 1 2]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "lr_clf = LogisticRegression(solver='liblinear')\n",
    "lr_clf.fit(x_train , y_data)\n",
    "pred = lr_clf.predict(x_train)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "83988a4d-cb7f-4cf5-842a-ef29d6a91fcb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (1, 28)\t1\n",
      "  (1, 29)\t1\n",
      "  (1, 50)\t1\n",
      "  (1, 51)\t1\n",
      "  (1, 67)\t1\n",
      "  (1, 80)\t1\n",
      "  (1, 81)\t1\n",
      "  (1, 82)\t1\n",
      "  (1, 83)\t1\n",
      "  (2, 55)\t1\n",
      "  (3, 34)\t1\n",
      "  (3, 37)\t1\n",
      "  (3, 44)\t1\n",
      "  (3, 45)\t1\n",
      "  (3, 53)\t1\n",
      "  (3, 63)\t1\n",
      "  (3, 64)\t1\n",
      "[[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0\n",
      "  0 1 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0\n",
      "  0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "(4, 103)\n"
     ]
    }
   ],
   "source": [
    "val = cvt.transform(['알수 없는 내용','뽀로로 자수네임택 어린이집','산리오','시나모롤 사탕'])\n",
    "print(val)\n",
    "print(val.todense())\n",
    "print(val.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "fab17fa4-9bbd-41e5-a503-45e499591017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2 0 2 1]\n"
     ]
    }
   ],
   "source": [
    "pred = lr_clf.predict(val)\n",
    "print(pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
